{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34986e0d-8c8c-4916-8be7-61e74a2cfe36",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a111ac3e-43d9-432b-8557-2b603313f414",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages (from gym[box2d]) (1.23.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages (from gym[box2d]) (2.2.0)\n",
      "Requirement already satisfied: box2d-py==2.3.5 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from gym[box2d]) (2.3.5)\n",
      "Requirement already satisfied: pyglet>=1.4.0 in c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages (from gym[box2d]) (1.5.27)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages (from matplotlib) (1.23.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (1.13.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from torch) (4.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (4.64.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\love\\anaconda3\\envs\\comp3340\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ox2d-py (c:\\users\\love\\appdata\\roaming\\python\\python38\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym[box2d] --user\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa56b55f-2363-4b4b-884d-97aa0dda4ebe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd784693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "%matplotlib inline\n",
    "\n",
    "# Pytorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as D\n",
    "\n",
    "# Use GPU is possible else use CPU\n",
    "device = torch.device('cpu')\n",
    "if(torch.cuda.is_available()): \n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27ab7d9-5f0d-4c2b-8f9b-e33a5689c0e8",
   "metadata": {},
   "source": [
    "## Test The Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "644ba50e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00338373  1.4144926  -0.3427514   0.15877305  0.00392769  0.07763831\n",
      "  0.          0.        ]\n",
      "Sample Action:  [-0.56501263 -0.9725253 ]\n",
      "Obsevation Space Action:  (8,)\n",
      "Sample Observation:  [ 0.62856275  1.1637546   0.57594156  1.7596751   1.9705803   1.3826134\n",
      " -0.99700797 -0.6544249 ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "state = env.reset()\n",
    "print(state)\n",
    "print(\"Sample Action: \", env.action_space.sample())\n",
    "print(\"Obsevation Space Action: \", env.observation_space.shape)\n",
    "print(\"Sample Observation: \", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca83f86-75d4-4591-8e44-653486865332",
   "metadata": {},
   "source": [
    "## Test With Random Walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b171f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes: 0 Score -349.5996386662349\n",
      "Next state: [-1.0066665  0.7347164 -1.8781767 -0.9616197  1.2742034  0.4709881\n",
      "  0.         0.       ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 1 Score -305.0449154537938\n",
      "Next state: [ 0.08544865  0.0081467   1.3655968   0.12027487 -2.038954   -1.4922372\n",
      "  1.          0.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 2 Score -188.0681845213545\n",
      "Next state: [ 0.6013529  -0.14706254  1.3968407  -0.7038593  -0.365758    2.090886\n",
      "  1.          1.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 3 Score -241.20542876323051\n",
      "Next state: [ 0.89951193  0.1705021   0.560556    0.32094297 -1.3890476  -1.1396773\n",
      "  1.          0.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 4 Score -379.90295297848735\n",
      "Next state: [-1.0117997   0.3439705  -1.9118421  -1.2424201   1.2993275   0.26892695\n",
      "  0.          0.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 5 Score -53.45371918525028\n",
      "Next state: [-0.3593564  -0.07387808 -0.4460289  -0.6127158   0.02313594  3.38465\n",
      "  1.          1.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 6 Score -75.35577725291478\n",
      "Next state: [ 0.6153652   0.07608727  0.48525923 -0.5404445   0.07986598  4.442504\n",
      "  1.          0.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 7 Score -370.5436364503204\n",
      "Next state: [-0.8465205   0.21232739 -1.5942247   0.04588151  2.2190633   2.6491327\n",
      "  0.          0.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 8 Score -112.77112925052145\n",
      "Next state: [-3.3183947e-01 -6.3589372e-02 -6.8933785e-01 -6.0676098e-01\n",
      "  1.2104884e-03  3.7339001e+00  1.0000000e+00  0.0000000e+00], reward: -100, done: True, info {}\n",
      "\n",
      "\n",
      "Episodes: 9 Score -143.48655138245124\n",
      "Next state: [-0.2515325  -0.0225568   0.07048623  0.04717106 -1.5515888  -0.29968938\n",
      "  1.          0.        ], reward: -100, done: True, info {}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for ep in range(episodes):\n",
    "    \n",
    "    states = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        # print('Action Taken {}'.format(action))\n",
    "        # https://stackoverflow.com/questions/73195438/openai-gyms-env-step-what-are-the-values\n",
    "        next_state, reward, done , info = env.step(action)\n",
    "        score += reward\n",
    "        \n",
    "    print('Episodes: {} Score {}'.format(ep, score))\n",
    "    print('Next state: {}, reward: {}, done: {}, info {}'.format(next_state, reward, done, info))\n",
    "    print('\\n')\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4b62502-7814-4c0c-93af-fe04dec326bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2850,  0.0501, -0.1043,  0.8141, -1.5025, -1.8337, -0.5464, -0.6183]])\n"
     ]
    }
   ],
   "source": [
    "state = torch.FloatTensor(env.observation_space.sample()).to(\"cpu\").unsqueeze(0)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299055a-4872-4d70-92e0-913cdc10972a",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d8a77d3-f798-4328-a6d1-11145c397489",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        super(ReplayBuffer, self).__init__()\n",
    "        self.memory = []\n",
    "    # Add replay memory\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        # print('action: {}'.format(action))\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self):\n",
    "        batch = self.memory\n",
    "        '''\n",
    "        np.stack: Join a sequence of arrays along a new axis.\n",
    "        zip: returns a zip object, an iterator of tuples where the first item in each passed iterator is paired together, \n",
    "             and then the second item in each passed iterator are paired together etc.\n",
    "        '''\n",
    "        states, actions, rewards, next_states, dones = map(np.stack, zip(*batch))\n",
    "        # Return the a set of trajectories of state, action, reward, next_state, done\n",
    "        return  states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def reset(self):\n",
    "        self.memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a71f5eb0-e5b4-4ce5-b718-80c8e142b78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinousPolicyNet(nn.Module):\n",
    "    def __init__(self, state_num, min_action, max_action):\n",
    "        super(ContinousPolicyNet, self).__init__()\n",
    "        self.min_action = min_action\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        self.input = nn.Linear(state_num, 32)\n",
    "        self.mu = nn.Linear(32, 2)\n",
    "        self.std = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        mu = (self.max_action - self.min_action) * torch.sigmoid(self.mu(x)) + self.min_action\n",
    "        std = (self.max_action - self.min_action) * torch.sigmoid(self.std(x)) / 2\n",
    "        return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "040bb157-60ee-4372-9ddb-0132d7cb81cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, state_num):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.input = nn.Linear(state_num, 32)\n",
    "        self.output = nn.Linear(32, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input(x))\n",
    "        value = self.output(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4281b148-ecd0-444c-b05d-98f2c1b6e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRPO():\n",
    "    # Gamma: Discount Factor, Delta: Gradient Descent Learning Rule\n",
    "    def __init__(self, env, gamma=0.99, learning_rate=1e-3, delta=0.05):\n",
    "        self.env = env\n",
    "        '''\n",
    "        Observation Space = 8: \n",
    "        1&2: coordinates of the lander in `x` & `y`\n",
    "        3&4: linear velocities in `x` & `y`\n",
    "        5&6: angle, angular velocity,\n",
    "        7&8: two booleans that represent whether each leg is in contact with the ground or not\n",
    "        Action Max = 1\n",
    "        Action Min = -1\n",
    "        ''' \n",
    "        self.state_num = self.env.observation_space.shape[0]\n",
    "        self.action_max = float(env.action_space.high[0])\n",
    "        self.action_min = float(env.action_space.low[0])\n",
    "               \n",
    "        # Torch\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Discount Factor setting\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Constraint\n",
    "        self.delta = delta\n",
    "        \n",
    "        # Policy (actor)\n",
    "        self.actor_net = ContinousPolicyNet(self.state_num, self.action_min, self.action_max).to(self.device)\n",
    "        \n",
    "        # Critic\n",
    "        self.critic_net = CriticNet(self.state_num).to(self.device)\n",
    "        '''\n",
    "        Adam is a first-order optimizers\n",
    "        https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "        optima.Adam returns Î¸t\n",
    "        '''\n",
    "        self.critic_opt = optim.Adam(self.critic_net.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Rollout\n",
    "        self.memory = ReplayBuffer()\n",
    "    \n",
    "    # Get an action\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(state).to(self.device).unsqueeze(0)\n",
    "        mu, std = self.actor_net(state)\n",
    "        \n",
    "        # print('mu: {}, std:{}'.format(mu.cpu().detach().numpy(),std.cpu().detach().numpy()))\n",
    "        \n",
    "        # first_action = D.Normal(mu, std).sample()\n",
    "        # second_action = D.Normal(mu, std).sample()\n",
    "        # first_action = first_action.cpu().detach().numpy()\n",
    "        # second_action = second_action.cpu().detach().numpy()\n",
    "        # action = [[first_action[0][0], second_action[0][0]]]\n",
    "        action = D.Normal(mu, std).sample()\n",
    "        action = action.cpu().detach().numpy()\n",
    "        return action[0]\n",
    "    \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "\n",
    "    # Flatten a gradient\n",
    "    def flat_grad(self, y, x, retain_graph=False, create_graph=False):\n",
    "        retain_graph = True if create_graph == True else retain_graph\n",
    "        '''\n",
    "        autograd.grad: Computes and returns the sum of gradients of outputs with respect to the inputs\n",
    "        https://pytorch.org/docs/stable/generated/torch.autograd.grad.html\n",
    "        \n",
    "        cat: Concatenates the given sequence of tensors in the given dimension\n",
    "        https://pytorch.org/docs/stable/generated/torch.cat.html\n",
    "        '''\n",
    "        grad = torch.autograd.grad(y, x, retain_graph=retain_graph, create_graph=create_graph)\n",
    "        grad = torch.cat([t.view(-1) for t in grad])\n",
    "        return grad\n",
    "    \n",
    "    # Hessian vector product\n",
    "    def hvp(self, d_kl, v, params, retain_graph):\n",
    "        return self.flat_grad(d_kl @ v, params, retain_graph)\n",
    "    \n",
    "    # Conjugate gradient to calculate Ax = b\n",
    "    def conjugate_gradient(self, A, d_kl, params, retain_graph, b, max_iterations=10):\n",
    "        x = torch.zeros_like(b)\n",
    "        r = b.clone() # b - Ax\n",
    "        v = r.clone() # r\n",
    "        \n",
    "        for _ in range(max_iterations):\n",
    "            Av = A(d_kl, v, params, retain_graph)\n",
    "            alpha = (r @ r) / (v @ Av)\n",
    "            x_new = x + alpha * v\n",
    "            r = r - alpha * Av\n",
    "            v = r - (r @ Av) / (v @ Av) * v\n",
    "            x = x_new\n",
    "        return x\n",
    "    \n",
    "    # Surrogate objective for maximizing\n",
    "    def surrogate_objective(self, log_prob_old, log_prob_new, advantages):\n",
    "        objective = advantages * torch.exp(log_prob_new - log_prob_old)\n",
    "        return objective.mean()\n",
    "       # KL divergence\n",
    "    def kl_divergence(self, mu_old, std_old, logstd_old, mu_new, std_new, logstd_new):\n",
    "        kl = (logstd_old - logstd_new) + (std_old.pow(2) + (mu_old - mu_new).pow(2)) / (2.0 * std_new.pow(2)) - 0.5\n",
    "        return kl.sum()\n",
    "    \n",
    "    # Flatten a gradient\n",
    "    def flat_grad(self, y, x, retain_graph=False, create_graph=False):\n",
    "        retain_graph = True if create_graph == True else retain_graph\n",
    "        grad = torch.autograd.grad(y, x, retain_graph=retain_graph, create_graph=create_graph)\n",
    "        grad = torch.cat([t.view(-1) for t in grad])\n",
    "        return grad\n",
    "    \n",
    "    # Update a parameter from flattend gradient\n",
    "    def param_update(self, policy_net, flattened_grad):\n",
    "        index = 0\n",
    "        for param in policy_net.parameters():\n",
    "            param_length = param.numel()\n",
    "            grad = flattened_grad[index : index+param_length].view(param.shape)\n",
    "            param.data += grad\n",
    "            index += param_length\n",
    "            \n",
    "    \"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "    def learn(self):\n",
    "        # Get memory from rollout\n",
    "        states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "        states = torch.FloatTensor(states).to(self.device)\n",
    "        actions = torch.FloatTensor(actions).to(self.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
    "        next_state = torch.FloatTensor(next_states[-1]).to(self.device)\n",
    "        done = dones[-1]\n",
    "        \n",
    "        # Critic network\n",
    "        values = self.critic_net(states)\n",
    "        next_value = self.critic_net(next_state)\n",
    "        \n",
    "        # Calculate target values and advantages\n",
    "        R = [0] * (actions.size(dim=0) + 1)\n",
    "        R[-1] = next_value if not done else 0\n",
    "        for i in reversed(range(len(R)-1)):\n",
    "            R[i] = rewards[i] + self.gamma * R[i+1]\n",
    "        R = torch.FloatTensor(R[:-1]).to(self.device).view(-1,1)\n",
    "        \n",
    "        # Calculate and normalize advantages to reduce skewness and improve convergence\n",
    "        advantages = R.detach() - values\n",
    "        advantages = ((advantages - advantages.mean()) / advantages.std()).view(1, -1) if len(advantages) > 1 else advantages\n",
    "\n",
    "        # Calculate critic losses and optimize the critic network\n",
    "        critic_loss = 0.5 * advantages.pow(2).mean()\n",
    "        self.critic_opt.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_opt.step()\n",
    "        \n",
    "        # Get pi theta old\n",
    "        mu_old, std_old = self.actor_net(states)\n",
    "        dist_old = D.Normal(mu_old, std_old)\n",
    "        \n",
    "        '''\n",
    "        Error here, matrix size problem\n",
    "        '''\n",
    "        log_probs_old = dist_old.log_prob(actions)\n",
    "        \n",
    "        # Compute L and KL\n",
    "        L_old = self.surrogate_objective(log_probs_old.detach(), log_probs_old, advantages)\n",
    "        KL_old = self.kl_divergence(mu_old.detach(), std_old.detach(), log_probs_old.detach(), mu_old, std_old, log_probs_old)\n",
    "        \n",
    "        # Policy network parameters\n",
    "        params = list(self.actor_net.parameters())\n",
    "\n",
    "        # Set the g and kl gradient\n",
    "        g = self.flat_grad(L_old, params, retain_graph=True)\n",
    "        d_kl = self.flat_grad(KL_old, params, create_graph=True)\n",
    "        \n",
    "        # s ia a search direction and beta is a maximal step length\n",
    "        s = self.conjugate_gradient(self.hvp, d_kl, params, True, g)\n",
    "        beta = torch.sqrt(2 * self.delta / (s @ self.hvp(d_kl, s, params, True)))\n",
    "        max_step = beta * s\n",
    "\n",
    "        # Line search\n",
    "        for i in range(10):\n",
    "            # Set the step size\n",
    "            step = (0.9 ** i) * max_step\n",
    "            \n",
    "            # Apply parameters' update\n",
    "            self.param_update(self.actor_net, step)\n",
    "\n",
    "            with torch.no_grad():            \n",
    "                # Get pi theta new after updating the network\n",
    "                mu_new, std_new = self.actor_net(states)\n",
    "                dist_new = D.Normal(mu_new, std_new)\n",
    "                log_probs_new = dist_new.log_prob(actions)\n",
    "                \n",
    "                # Compute L and KL after updating the network\n",
    "                L_new = self.surrogate_objective(log_probs_old.detach(), log_probs_new, advantages)\n",
    "                KL_new = self.kl_divergence(mu_old.detach(), std_old.detach(), log_probs_old.detach(), mu_new, std_new, log_probs_new)\n",
    "\n",
    "            # Calculate the improvement of objective value\n",
    "            L_improvement = L_new - L_old\n",
    "            \n",
    "            # If the improvement of L is positive and the kl value is lower than delta, fix the parameters\n",
    "            if L_improvement > 0 and KL_new <= self.delta:\n",
    "                break\n",
    "            \n",
    "            # Else, reset the parameters\n",
    "            self.param_update(self.actor_net, -step)\n",
    "            \n",
    "        # Reset the memory\n",
    "        self.memory.reset()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86499898-5598-4808-a246-962eb0d9e5a3",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eafee83b-242c-4126-b2e1-1a1bb5f67975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('LunarLanderContinuous-v2')\n",
    "\n",
    "    agent = TRPO(env, gamma=0.99, learning_rate=1e-6, delta=0.01)\n",
    "    ep_rewards = deque(maxlen=20)\n",
    "    total_episode = 1\n",
    "    \n",
    "    state = env.reset()\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done , _ = env.step(action)\n",
    "    print(action)\n",
    "    print(next_state)\n",
    "    print(reward, done)\n",
    "\n",
    "    for i in range(total_episode):\n",
    "        state = env.reset()\n",
    "        rewards = []\n",
    "        \n",
    "        while True:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done , _ = env.step(action)\n",
    "            agent.memory.add(state, action, reward, next_state, done)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                agent.learn()\n",
    "                ep_rewards.append(sum(rewards))\n",
    "                if i % 20 == 0:\n",
    "                    print(\"episode: {}\\treward: {}\".format(i, round(np.mean(ep_rewards), 3)))\n",
    "                break\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec73b2-231d-4cd5-b466-0fb1935083a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9790e176aa061e7232846136379112eabeee40f35a9f008e303bdf6034c5f371"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
